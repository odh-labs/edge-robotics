{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e260f146-9844-4854-a29c-7ef08829e4e7",
   "metadata": {},
   "source": [
    "# Export YoloV5 model to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c563e-535f-431f-b49c-b2008afacf6f",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a11749-e831-4c36-8dd5-ebad7954b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you are getting \"ImportError: libGL.so.1: cannot open shared object file: No such file or directory\"\n",
    "#!pip uninstall opencv-python-headless cv2 --yes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20ffb88-1685-4448-9af4-a981d83e94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df2bcd-d5e2-4fa3-8b68-bf8270d021d3",
   "metadata": {},
   "source": [
    "## Download model\n",
    "You can download the model of your choice (different sizes and characteristics) from here: https://github.com/ultralytics/yolov5#pretrained-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24297e76-2a68-4885-a070-1880eac3f902",
   "metadata": {},
   "source": [
    "## Export the model to ONNX and then to Openvino\n",
    "- Eventually modify the image size if you selected a model with the \"6\" suffix, as image size is 1280 (and not 640)\n",
    "- Stay at opset 16 for the moment for OpenVino model server compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3c4258-44f8-41ef-8d9f-499daea6a9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['../yolov5s.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=16, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx', 'openvino']\n",
      "YOLOv5 ðŸš€ 2023-9-14 Python-3.11.4 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 3089188 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from ../yolov5s.pt with output shape (1, 25500, 7) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1...\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.6s, saved as ../yolov5s.onnx (12.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2023.0.2-11065-e662b1a3301-releases/2023/0...\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /Users/arslankhan/Codes/edge-robotics/model-lean/yolov5s_openvino_model/yolov5s.xml\n",
      "[ SUCCESS ] BIN file: /Users/arslankhan/Codes/edge-robotics/model-lean/yolov5s_openvino_model/yolov5s.bin\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success âœ… 3.4s, saved as ../yolov5s_openvino_model/ (12.4 MB)\n",
      "\n",
      "Export complete (5.5s)\n",
      "Results saved to \u001b[1m/Users/arslankhan/Codes/edge-robotics/model-lean\u001b[0m\n",
      "Detect:          python detect.py --weights ../yolov5s_openvino_model/ \n",
      "Validate:        python val.py --weights ../yolov5s_openvino_model/ \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '../yolov5s_openvino_model/')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights ../yolov5s.pt --include onnx openvino --imgsz 640 --opset 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe3a98",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "### For inferencing \n",
    "For inferencing see 230 on https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks\n",
    "Also relevant are 102 and 121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e618842",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
